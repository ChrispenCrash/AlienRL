{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlienRL: Exploring the use of Proximal Policy Optimization in a real-time car racing simulator\n",
    "This notebook is contains the code from train_mt.py, broken up into separate cells. <br>\n",
    "Correct implementation of this code requires activating the conda environment \"torch-gpu\" included in the .yml file in the main directory.<br>\n",
    "and running the command `python train_mt.py`. <br>\n",
    "Assetto Corsa running in windowed mode of 1280x720 is also required for this code to run correctly.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# import logging\n",
    "run_start_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# log_file_path = f'logs/{run_start_time}_log.txt'\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from PPO.PPO import PPO\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import copy\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "\n",
    "from AlienEnv.alienrl_env import AlienRLEnv\n",
    "from AlienEnv.utils import ActionSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"AlienRLEnv\"\n",
    "\n",
    "env = AlienRLEnv()\n",
    "\n",
    "# Not implemented in current version, currently, batch_size = buffer_size\n",
    "batch_size = 1024\n",
    "\n",
    "max_training_timesteps = 10_000_000  # break training loop if timesteps > max_training_timesteps\n",
    "\n",
    "print_freq = 1 # batch_size * 10\n",
    "save_model_freq = 25_000 #\n",
    "\n",
    "# Starting standard deviation for action distribution\n",
    "action_sd = 0.6\n",
    "# Linearly decay action_sd where, action_sd = action_sd - action_sd_decay_rate\n",
    "action_sd_decay_rate = 0.05        \n",
    "# Set minimum action standard deviation\n",
    "min_action_sd = 0.1                \n",
    "\n",
    "# action standard devation decay frequency\n",
    "action_sd_decay_freq = 1_000_000 # 250_000\n",
    "\n",
    "ent_coef_decay_freq = 1_000_000\n",
    "ent_coef_decay_rate = 10\n",
    "min_ent_coef_cutoff = 0.001\n",
    "\n",
    "# Batch/buffer size for training, should be multiple of batch_size\n",
    "# buffer_size = batch_size * 1  # 1024 - Converged faster, at 300k timesteps (ent_coef = 0.0)\n",
    "buffer_size = batch_size * 4  # 4096 - Converged at 500k timesteps (ent_coef = 0.001)\n",
    "# buffer_size = batch_size * 40 # 40960 - Converges at much slower rate and stable rate\n",
    "\n",
    "# Update policy for n epochs\n",
    "num_of_epochs = 128 # 128 # 80 # 10\n",
    "\n",
    "eps_clip = 0.3\n",
    "gamma = 0.99\n",
    "lr_actor = 0.0003\n",
    "lr_critic = 0.0003\n",
    "ent_coef = 0.01 # 0.001 # 0.001 # Increasing entropy coefficient helps exploration, 0 seems to be the best value\n",
    "vf_coef = 0.5\n",
    "\n",
    "state_dim = sum(env.observation_space['framestack'].shape) + env.observation_space['telemetry'].shape[0]\n",
    "\n",
    "# action space dimension\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "checkpoint_path = \"models\" + '/' + f\"{run_start_time}_{ent_coef}\" + \"/\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "logs_dir = f\"runs/{run_start_time}_{ent_coef}\"\n",
    "\n",
    "writer = SummaryWriter(logs_dir)\n",
    "\n",
    "# initialize a PPO agent\n",
    "agent = PPO(state_dim, action_dim, batch_size, buffer_size, lr_actor, lr_critic, gamma, num_of_epochs, eps_clip, ent_coef, vf_coef, action_sd)\n",
    "\n",
    "agent.load(\"best_model.pth\")\n",
    "\n",
    "print(\"Initialisation complete.\")\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Training started at: \", start_time)\n",
    "\n",
    "# printing and logging variables\n",
    "total_episodes = 0\n",
    "\n",
    "global_step_num = 0\n",
    "episode_num = 1\n",
    "\n",
    "best_reward = env.reward_range[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []\n",
    "episode_times = []\n",
    "agent_update_times = []\n",
    "\n",
    "def update_agent(agent):\n",
    "    agent.update()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "\n",
    "    future = None\n",
    "\n",
    "    smoother = ActionSmoother(alpha=0.8)\n",
    "\n",
    "    env.controller.set_inputs(0.0,0.5)\n",
    "\n",
    "    # training loop\n",
    "    while global_step_num <= max_training_timesteps:\n",
    "\n",
    "        episode_start_time = datetime.now().replace(microsecond=0)\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        trunc = False\n",
    "\n",
    "        while not done and not trunc:\n",
    "            \n",
    "            # Collect updated agent\n",
    "            if future and future.done():\n",
    "                agent_update_finish_time = datetime.now()\n",
    "                agent_update_times.append(agent_update_finish_time - agent_update_start_time)\n",
    "                agent = future.result()\n",
    "                print(f\"Agent updated.\")\n",
    "                agent.buffer.clear()\n",
    "                print(f\"Buffer cleared.\")\n",
    "                future = None\n",
    "            \n",
    "            action = agent.select_action(state)\n",
    "            smoothed_action = smoother.smooth(action)\n",
    "\n",
    "            state, reward, done, trunc, info = env.step(smoothed_action)\n",
    "\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.is_terminals.append(done or trunc)\n",
    "\n",
    "            global_step_num += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update agent\n",
    "            if len(agent.buffer.rewards) == buffer_size:\n",
    "                if not future:\n",
    "                    agent_update_start_time = datetime.now()\n",
    "                    print(\"Updating agent...\")\n",
    "                    # t4 = datetime.now()\n",
    "                    new_agent = copy.deepcopy(agent)\n",
    "                    t6 = datetime.now()\n",
    "                    print(f\"Agent copied.\")\n",
    "                    future = executor.submit(update_agent, new_agent) # update the new agent asynchronously\n",
    "\n",
    "            if global_step_num % action_sd_decay_freq == 0:\n",
    "                agent.decay_action_sd(action_sd_decay_rate, min_action_sd)\n",
    "\n",
    "            if global_step_num % ent_coef_decay_freq == 0:\n",
    "                # Should go to 0.001 after 1M steps, 0 after 2M steps\n",
    "                agent.decay_ent_coef(ent_coef_decay_rate, min_ent_coef_cutoff)\n",
    "\n",
    "            if global_step_num % save_model_freq == 0:\n",
    "                print(\"Saving model...\")\n",
    "                agent.save(f\"{checkpoint_path}{global_step_num}.pth\")\n",
    "\n",
    "\n",
    "        episode_times.append(datetime.now().replace(microsecond=0) - episode_start_time)\n",
    "        reward_history.append(episode_reward)\n",
    "\n",
    "        avg_episode_time = np.mean(episode_times[-100:]).total_seconds()\n",
    "        avg_reward = np.mean(reward_history[-100:])\n",
    "\n",
    "        if len(agent_update_times) > 0:\n",
    "            avg_agent_update_time = np.mean(agent_update_times[-100:]).total_seconds()\n",
    "        else:\n",
    "            avg_agent_update_time = 0\n",
    "\n",
    "        if avg_reward > best_reward and len(reward_history) >= 100:\n",
    "            best_reward = avg_reward\n",
    "            \n",
    "        \n",
    "        if (total_episodes+1) % print_freq == 0:\n",
    "            print(\"============================================================================================\")\n",
    "            print(f\"Episode: {episode_num} \\t Total Steps: {global_step_num} \\t Average Reward: {avg_reward:9.02f} \\t Best Reward: {best_reward:.02f}\") \n",
    "            print(f\"Elapsed Time: {datetime.now().replace(microsecond=0) - start_time} \\t Avg Episode Time: {avg_episode_time:.2f}s \\t Avg Agent Update Time: {avg_agent_update_time:.2f}s\")\n",
    "            print(\"============================================================================================\")\n",
    "\n",
    "        writer.add_scalar('Reward', episode_reward, global_step=global_step_num)\n",
    "        writer.add_scalar('Average Reward', avg_reward, global_step=global_step_num)\n",
    "        writer.add_scalar('Agent Train Time(s)', avg_agent_update_time, global_step=global_step_num)\n",
    "\n",
    "        total_episodes += 1\n",
    "\n",
    "        episode_num += 1\n",
    "\n",
    "writer.close()\n",
    "env.close()\n",
    "\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print()\n",
    "print(\"Started training at: \", start_time)\n",
    "print(\"Finished training at: \", end_time)\n",
    "print(\"Total training time: \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
